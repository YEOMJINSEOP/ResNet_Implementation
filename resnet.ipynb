{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 Arguments\n",
    "## pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "## progress (bool): If True, displays a progress bar of the download to stderr\n",
    "\n",
    "# basic ResNet 34 layer\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "  return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "  \n",
    "# basic ResNet 50 layer\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "  return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    r\"\"\"\n",
    "    - pretrained: pretrained된 모델 가중치를 불러오기 (saved by caffe)\n",
    "    - arch: ResNet모델 이름\n",
    "    - block: 어떤 block 형태 사용할지 (\"Basic or Bottleneck\")\n",
    "    - layers: 해당 block이 몇번 사용되는지를 list형태로 넘겨주는 부분\n",
    "    \"\"\"\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, groups=1, dilation=1):\n",
    "    r\"\"\"\n",
    "    3x3 convolution with padding\n",
    "    - bias=False: BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정.\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=dilation, groups=groups, \n",
    "                     bias=False, dilation=dilation)\n",
    "    \n",
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                     stride=stride, bias=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks: BasicBlock, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        r\"\"\"\n",
    "         - groups, base_width: ResNext나 Wide ResNet의 경우 사용\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "            \n",
    "        # Basic Block의 구조\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)  # conv1에서 downsample\n",
    "        self.bn1 = norm_layer(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = norm_layer(out_channels)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # short connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        # identity mapping시 identity mapping후 ReLU를 적용합니다.\n",
    "        # 그 이유는, ReLU를 통과하면 양의 값만 남기 때문에 Residual의 의미가 제대로 유지되지 않기 때문입니다.\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4 # 블록 내에서 차원을 증가시키는 3번째 conv layer에서의 확장계수\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 stride=1, downsample=None, groups=1, \n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "      \n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        # ResNext나 WideResNet의 경우 사용\n",
    "        \n",
    "        width = int(out_channels * (base_width / 64.)) * groups\n",
    "        \n",
    "        # Bottleneck Block의 구조\n",
    "        self.conv1 = conv1x1(in_channels, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        \n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # conv2에서 downsample\n",
    "        self.bn2 = norm_layer(width)\n",
    "        \n",
    "        self.conv3 = conv1x1(width, out_channels * self.expansion)\n",
    "        self.bn3 = norm_layer(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # 3x3 convolution layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        # skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, \n",
    "                 zero_init_residual=False, groups=1, width_per_group=64, \n",
    "                 replace_stride_with_dilation=None, norm_layer=None):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            \n",
    "        self._norm_layer = norm_layer\n",
    "        # default values\n",
    "        self.in_channels = 64 # input feature map\n",
    "        self.dilation = 1\n",
    "        \n",
    "        # stride를 dilation으로 대체할지 선택\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "            \n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "            \n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 처음 입력에 적용되는 self.conv1과 self.bn1, self.relu는 모든 ResNet에서 동일 \n",
    "        - 3: 입력으로 RGB 이미지를 사용하기 때문에 convolution layer에 들어오는 input의 channel 수는 3\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 아래부터 block 형태와 갯수가 ResNet층마다 변화\n",
    "        - self.layer1 ~ 4: 필터의 개수는 각 block들을 거치면서 증가(64->128->256->512)\n",
    "        - self.avgpool: 모든 block을 거친 후에는 Adaptive AvgPool2d를 적용하여 (n, 512, 1, 1)의 텐서로\n",
    "        - self.fc: 이후 fc layer를 연결\n",
    "        \"\"\"\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, # 여기서부터 downsampling적용\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1, dilate=False):\n",
    "        r\"\"\"\n",
    "        _make_layer함수는 논문의 conv2_X, conv3_x, conv4_X, conv5_x을 구현하며 각 층에 해당하는 block을 갯수에 맞게 생성 및 연결시켜주는 역할\n",
    "        \n",
    "        convolution layer 생성 함수\n",
    "        - block: block종류 지정\n",
    "        - out_channels: feature map size (input shape)\n",
    "        - blocks: layers[0]와 같이, 해당 블록이 몇개 생성돼야하는지, 블록의 갯수 (layer 반복해서 쌓는 개수)\n",
    "        - stride와 dilate은 고정\n",
    "        \"\"\"\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        \n",
    "        # the number of filters is doubled: self.in_channels와 out_channels 사이즈를 맞춰주기 위한 projection shortcut\n",
    "        # the feature map size is halved: stride=2로 downsampling\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_channels, out_channels * block.expansion, stride),\n",
    "                norm_layer(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # 블록 내 시작 layer, downsampling 필요\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.in_channels = out_channels * block.expansion # in_channels 업데이트\n",
    "        # 동일 블록 반복\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./dataset/', train=True, transform=transform, download=False)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./dataset/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34().to(device)\n",
    "\n",
    "# DataParallel 모듈로 모델 감싸기\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/500] Loss: 1.8273\n",
      "Epoch [1/10], Step [200/500] Loss: 1.6671\n",
      "Epoch [1/10], Step [300/500] Loss: 1.5635\n",
      "Epoch [1/10], Step [400/500] Loss: 1.2612\n",
      "Epoch [1/10], Step [500/500] Loss: 1.3407\n",
      "Epoch [2/10], Step [100/500] Loss: 1.2903\n",
      "Epoch [2/10], Step [200/500] Loss: 1.2313\n",
      "Epoch [2/10], Step [300/500] Loss: 1.4815\n",
      "Epoch [2/10], Step [400/500] Loss: 1.0947\n",
      "Epoch [2/10], Step [500/500] Loss: 1.1612\n",
      "Epoch [3/10], Step [100/500] Loss: 1.0263\n",
      "Epoch [3/10], Step [200/500] Loss: 1.1204\n",
      "Epoch [3/10], Step [300/500] Loss: 1.0741\n",
      "Epoch [3/10], Step [400/500] Loss: 1.1148\n",
      "Epoch [3/10], Step [500/500] Loss: 1.1461\n",
      "Epoch [4/10], Step [100/500] Loss: 0.9866\n",
      "Epoch [4/10], Step [200/500] Loss: 1.1636\n",
      "Epoch [4/10], Step [300/500] Loss: 0.9255\n",
      "Epoch [4/10], Step [400/500] Loss: 0.9639\n",
      "Epoch [4/10], Step [500/500] Loss: 0.8374\n",
      "Epoch [5/10], Step [100/500] Loss: 0.8364\n",
      "Epoch [5/10], Step [200/500] Loss: 0.8150\n",
      "Epoch [5/10], Step [300/500] Loss: 0.8778\n",
      "Epoch [5/10], Step [400/500] Loss: 0.8630\n",
      "Epoch [5/10], Step [500/500] Loss: 0.8586\n",
      "Epoch [6/10], Step [100/500] Loss: 0.8083\n",
      "Epoch [6/10], Step [200/500] Loss: 0.9446\n",
      "Epoch [6/10], Step [300/500] Loss: 1.0398\n",
      "Epoch [6/10], Step [400/500] Loss: 0.7541\n",
      "Epoch [6/10], Step [500/500] Loss: 0.9744\n",
      "Epoch [7/10], Step [100/500] Loss: 0.7881\n",
      "Epoch [7/10], Step [200/500] Loss: 0.9237\n",
      "Epoch [7/10], Step [300/500] Loss: 0.8079\n",
      "Epoch [7/10], Step [400/500] Loss: 0.6216\n",
      "Epoch [7/10], Step [500/500] Loss: 0.7480\n",
      "Epoch [8/10], Step [100/500] Loss: 0.8232\n",
      "Epoch [8/10], Step [200/500] Loss: 0.5719\n",
      "Epoch [8/10], Step [300/500] Loss: 0.7819\n",
      "Epoch [8/10], Step [400/500] Loss: 0.7089\n",
      "Epoch [8/10], Step [500/500] Loss: 0.7030\n",
      "Epoch [9/10], Step [100/500] Loss: 0.5649\n",
      "Epoch [9/10], Step [200/500] Loss: 0.9159\n",
      "Epoch [9/10], Step [300/500] Loss: 0.7425\n",
      "Epoch [9/10], Step [400/500] Loss: 0.6476\n",
      "Epoch [9/10], Step [500/500] Loss: 0.6237\n",
      "Epoch [10/10], Step [100/500] Loss: 0.6524\n",
      "Epoch [10/10], Step [200/500] Loss: 0.6334\n",
      "Epoch [10/10], Step [300/500] Loss: 0.6667\n",
      "Epoch [10/10], Step [400/500] Loss: 0.6530\n",
      "Epoch [10/10], Step [500/500] Loss: 0.6456\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader) # 500\n",
    "curr_lr = learning_rate # 0.001\n",
    "\n",
    "for epoch in range(num_epochs): #80\n",
    "  for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "  # Decay learning rate\n",
    "  if (epoch+1) % 20 == 0:\n",
    "    curr_lr /= 3\n",
    "    update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 73.39 %\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, dim=1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "  print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
